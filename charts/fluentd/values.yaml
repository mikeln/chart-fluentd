# Default values for fluentd.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
image:
  repository: gcr.io/google-containers/fluentd-elasticsearch
  tag: v2.3.1
  pullPolicy: IfNotPresent
  # pullSecrets:
  #   - secret1
  #   - secret2

output:
  host: elasticsearch
  port: 9200
  buffer_chunk_limit: 2M
  buffer_queue_limit: 8
  scheme: http

env: {}
# elasticsearchEnabled: false 
fluentESUser: elastic 
fluentESPassword: changeme 
fluentdPrivateKeyPassphrase: fbit 
awsSecKey: secretKey 
awsKeyId: keyId
s3Bucket: sampleBucketName
s3BucketPath: logs/
s3Region: sampleBucketRegion

enableTLS: false 

plugins: 
  - fluent-plugin-s3

service:
  type: ClusterIP
  externalPort: 80
  ports:
    - name: "monitor-agent"
      protocol: TCP
      containerPort: 24220

ingress:
  enabled: false
  # Used to create an Ingress and Service record.
  # hosts:
  #   - name: "http-input.local"
  #     protocol: TCP
  #     serviceName: http-input
  #     servicePort: 9880
  annotations:
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"

configMaps:
  general.conf: |
    # Prevent fluentd from handling records containing its own logs. Otherwise
    # it can lead to an infinite loop, when error in sending one message generates
    # another message which also fails to be sent and so on.
    <match fluentd.**>
      @type null
    </match>
    # Used for health checking
    <source>
      @type http
      port 9880
      bind 0.0.0.0
    </source>

  system.conf: |-
    <system>
      root_dir /tmp/fluentd-buffers/
    </system>
  forward-input.conf: |
    <source>
      @type dummy
      tag dummy 
      dummy {"hello":"mars"}
    </source>
    <source>
      @type forward
      @log_level info 
      port 24220
      # uncomment the following lines to use secure forward protocol , and turn enableTLS value to true
      # allow_self_signed_certificate true
      # <transport tls>
      #    ca_path                /fluentd/etc/ssl/ca.crt.pem
      #    cert_path              /fluentd/etc/ssl/server.crt.pem
      #    private_key_path       /fluentd/etc/ssl/server.key.pem
      #    private_key_passphrase "#{ENV['FLUENTD_PRIVATE_KEY_PASSPHRASE']}"
      #    client_cert_auth       true
      # </transport>
      # <security>
      #   self_hostname fluentd
      #   shared_key fluentd
      # </security>
      bind 0.0.0.0   #if using secure foward, comment out this line 
    </source>
    
  output.conf: |
    
    <match host.**>
      @type copy
      <store>
        @type elasticsearch
        @log_level info
        host localhost
        port 9200
        include_tag_key true
        tag_key @log_name
        logstash_format true
        logstash_prefix logsystemd
        flush_interval 10s
        max_retry_wait 30
        disable_retry_limit
        host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
        port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
        user "#{ENV['FLUENT_ELASTICSEARCH_USER']}"
        password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD']}"
      </store>
      <store>
        @type s3
        aws_key_id "#{ENV['AWS_KEY_ID']}"
        aws_sec_key "#{ENV['AWS_SECRET_KEY']}"
        s3_bucket "#{ENV['S3_BUCKET']}"
        s3_region "#{ENV['S3_REGION']}"
        auto_create_bucket false 
        path "#{ENV['S3_BUCKET_PATH']}"
        buffer_path /var/log/fluent/s3
        time_slice_format %Y%m%d%H
        time_slice_wait 2m
        utc
        buffer_chunk_limit 256m
      </store>
    </match>
    
    <match **>
      @type copy
      <store>
        @type elasticsearch
        @log_level info
        host localhost
        port 9200
        include_tag_key true
        tag_key @log_name
        logstash_format true
        flush_interval 10s
        max_retry_wait 30
        disable_retry_limit
        host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
        port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
        user "#{ENV['FLUENT_ELASTICSEARCH_USER']}"
        password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD']}"
      </store>
      <store>
        @type s3
        aws_key_id "#{ENV['AWS_KEY_ID']}"
        aws_sec_key "#{ENV['AWS_SECRET_KEY']}"
        s3_bucket "#{ENV['S3_BUCKET']}"
        s3_region "#{ENV['S3_REGION']}"
        auto_create_bucket false
        path "#{ENV['S3_BUCKET_PATH']}"
        buffer_path /var/log/fluent/s3
        time_slice_format %Y%m%d%H
        time_slice_wait 2m
        utc
        buffer_chunk_limit 256m
      </store>
    </match>
resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #  cpu: 500m
  #  memory: 200Mi
  # requests:
  #  cpu: 500m
  #  memory: 200Mi

## Persist data to a persistent volume
persistence:
  enabled: false

  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  # storageClass: "-"
  # annotations: {}
  accessMode: ReadWriteOnce
  size: 10Gi

nodeSelector: {}

tolerations: []

affinity: {}
